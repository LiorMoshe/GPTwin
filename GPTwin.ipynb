{"cells":[{"cell_type":"markdown","metadata":{"id":"Ko3J-uXeg2T3"},"source":["## Using Gradio to wrap a text to text interface around GPT-2 \n","\n","Check out the library on [github](https://github.com/gradio-app/gradio-UI) and see the [getting started](https://gradio.app/getting_started.html) page for more demos."]},{"cell_type":"markdown","metadata":{"id":"npQy3li2g923"},"source":["### Installs and Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NfR2g52--Gus"},"outputs":[],"source":["!pip install -q gradio\n","!pip install -q transformers\n","!pip install -q Tokenizers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fpnqjQor-Guv"},"outputs":[],"source":["import gradio as gr\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import pandas as pd\n","import random\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n","from tqdm import tqdm, trange\n","import torch.nn.functional as F\n","import csv\n","import os"]},{"cell_type":"markdown","metadata":{"id":"LdTngat-hNoV"},"source":["### Loading the model and creating the generate function"]},{"cell_type":"markdown","metadata":{"id":"Kqwhb-aLh6QL"},"source":["Note: You can also change to `hebrew-gpt_neo-tiny`, `hebrew-gpt_neo-small` or `hebrew-gpt_neo-xl`\n","\n","---\n","\n","\n","\n","> Indented block\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":273},"executionInfo":{"elapsed":56107,"status":"ok","timestamp":1663684912805,"user":{"displayName":"Lior Moshe","userId":"15921272631253802183"},"user_tz":-180},"id":"V0PzPkvC-Guy","outputId":"9943c48d-5cb9-4548-865f-cd67607f4f60"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/tokenizer.json\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/config.json\n","Model config GPTNeoConfig {\n","  \"_name_or_path\": \"Norod78/hebrew-gpt_neo-small\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPTNeoForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0,\n","  \"attention_layers\": [\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\"\n","  ],\n","  \"attention_types\": [\n","    [\n","      [\n","        \"global\"\n","      ],\n","      12\n","    ]\n","  ],\n","  \"bos_token_id\": 50256,\n","  \"embed_dropout\": 0,\n","  \"eos_token_id\": 50256,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": null,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"gpt_neo\",\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"pad_token_id\": 2,\n","  \"resid_dropout\": 0,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.22.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257,\n","  \"window_size\": 256\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n","\n","All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at Norod78/hebrew-gpt_neo-small.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n"]}],"source":["#model_name = \"Norod78/hebrew-gpt_neo-tiny\"\n","model_name = \"Norod78/hebrew-gpt_neo-small\"\n","\n","# Use only out of collab on local monster of a machine, collab doesn't have enough RAM.\n","# model_name = \"Norod78/hebrew-gpt_neo-xl\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)"]},{"cell_type":"markdown","metadata":{"id":"CiqTyzRPNz_7"},"source":["Scrape Data From Twitter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4b5EH7UXOFe9"},"outputs":[],"source":["from google.colab import drive\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9ywByCdPNWR"},"outputs":[],"source":["twts =  open(\"/content/drive/My Drive/data.txt\").read().split('END$')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1663684942451,"user":{"displayName":"Lior Moshe","userId":"15921272631253802183"},"user_tz":-180},"id":"ZlG5Kw24PkRL","outputId":"c47be60a-9c4f-4b77-97e8-3089fa518dda"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'הרגע יצאתי באמצע התוכנית של אופירה וברקוביץ. עשו עלי אמבוש. הכניסו בחור שלא סובל אותי שילכלכך עלי בפריים טיים.\\n\\nכל היום התכוננתי לראיון איתם, ישבתי בבית ועברתי על טקסטים. באמת זה היה לי חשוב.\\n\\nניראה אותם עושים את זה לאיילת שקד. \\nאבל היי, לרמוס את הדר מוכתר בשידור- מביא רייטינג. '"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["twts[4]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1663684943195,"user":{"displayName":"Lior Moshe","userId":"15921272631253802183"},"user_tz":-180},"id":"X_fx1yztjeVG","outputId":"a94c2b62-5d4d-43f6-c049-1ed6fe18a0b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset length: 364\n","Test dataset length: 20\n"]}],"source":["import re\n","import json\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","def build_text_files(data_json, dest_path):\n","    f = open(dest_path, 'w')\n","    data = ''\n","    for texts in data_json:\n","        data += texts + \"  \"\n","    f.write(data)\n","\n","train, test = train_test_split(twts,test_size=0.05) \n","\n","\n","build_text_files(train,'train_dataset.txt')\n","build_text_files(test,'test_dataset.txt')\n","\n","print(\"Train dataset length: \"+str(len(train)))\n","print(\"Test dataset length: \"+ str(len(test)))"]},{"cell_type":"markdown","metadata":{"id":"6WE39qZwN3DP"},"source":["Tokenize the data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":671,"status":"ok","timestamp":1663684950872,"user":{"displayName":"Lior Moshe","userId":"15921272631253802183"},"user_tz":-180},"id":"3G3-dwYujfur","outputId":"0f21a589-719d-43d3-bd33-c97b4073b66a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","Token indices sequence length is longer than the specified maximum sequence length for this model (11727 > 1024). Running this sequence through the model will result in indexing errors\n"]}],"source":["from transformers import TextDataset,DataCollatorForLanguageModeling\n","\n","def load_dataset(train_path,test_path,tokenizer):\n","    train_dataset = TextDataset(\n","          tokenizer=tokenizer,\n","          file_path=train_path,\n","          block_size=128)\n","     \n","    test_dataset = TextDataset(\n","          tokenizer=tokenizer,\n","          file_path=test_path,\n","          block_size=128)   \n","    \n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer, mlm=False,\n","    )\n","    return train_dataset,test_dataset,data_collator\n","\n","train_dataset,test_dataset,data_collator = load_dataset('train_dataset.txt','test_dataset.txt',tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"3FRI59GFjyVz"},"source":["#Initialize the Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10436,"status":"ok","timestamp":1663684982533,"user":{"displayName":"Lior Moshe","userId":"15921272631253802183"},"user_tz":-180},"id":"2QCvNeO4j00p","outputId":"7607c1a5-5286-4f85-9bf0-eff60ddfeb81"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:1066: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n"]}],"source":["from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n","\n","model = AutoModelWithLMHead.from_pretrained(model_name)\n","\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./gpt2-muchtar-small\", #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=90, # number of training epochs\n","    per_device_train_batch_size=32, # batch size for training\n","    per_device_eval_batch_size=64,  # batch size for evaluation\n","    eval_steps = 400, # Number of update steps between two evaluations.\n","    save_steps=800, # after # steps model is saved \n","    warmup_steps=500,# number of warmup steps for learning rate scheduler\n","    prediction_loss_only=True,\n","    )\n","\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":272988,"status":"ok","timestamp":1663685282939,"user":{"displayName":"Lior Moshe","userId":"15921272631253802183"},"user_tz":-180},"id":"UFdh0ZnPkC4Z","outputId":"8984dc8b-e87c-4285-a11d-749f5bb47ea4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 91\n","  Num Epochs = 90\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 270\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [270/270 04:28, Epoch 90/90]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=270, training_loss=0.9737677680121528, metrics={'train_runtime': 271.9767, 'train_samples_per_second': 30.113, 'train_steps_per_second': 0.993, 'total_flos': 534821531811840.0, 'train_loss': 0.9737677680121528, 'epoch': 90.0})"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2459,"status":"ok","timestamp":1663439085642,"user":{"displayName":"Lior Moshe","userId":"15921272631253802183"},"user_tz":-180},"id":"RTQig2V_kTU_","outputId":"652a56c4-347b-4f75-c4ec-1b52faf754c2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./gpt2-muchtar-small\n","Configuration saved in ./gpt2-muchtar-small/config.json\n","Model weights saved in ./gpt2-muchtar-small/pytorch_model.bin\n"]}],"source":["trainer.save_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7147,"status":"ok","timestamp":1663439101356,"user":{"displayName":"Lior Moshe","userId":"15921272631253802183"},"user_tz":-180},"id":"VyvOMl-mkW31","outputId":"f9b70596-7eac-40d3-b992-bd26279cb364"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file ./gpt2-muchtar-small/config.json\n","Model config GPTNeoConfig {\n","  \"_name_or_path\": \"./gpt2-muchtar-small\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPTNeoForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0,\n","  \"attention_layers\": [\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\"\n","  ],\n","  \"attention_types\": [\n","    [\n","      [\n","        \"global\"\n","      ],\n","      12\n","    ]\n","  ],\n","  \"bos_token_id\": 50256,\n","  \"embed_dropout\": 0,\n","  \"eos_token_id\": 50256,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": null,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"gpt_neo\",\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"pad_token_id\": 50256,\n","  \"resid_dropout\": 0,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.22.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257,\n","  \"window_size\": 256\n","}\n","\n","loading configuration file ./gpt2-muchtar-small/config.json\n","Model config GPTNeoConfig {\n","  \"_name_or_path\": \"./gpt2-muchtar-small\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPTNeoForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0,\n","  \"attention_layers\": [\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\",\n","    \"global\"\n","  ],\n","  \"attention_types\": [\n","    [\n","      [\n","        \"global\"\n","      ],\n","      12\n","    ]\n","  ],\n","  \"bos_token_id\": 50256,\n","  \"embed_dropout\": 0,\n","  \"eos_token_id\": 50256,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": null,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"gpt_neo\",\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"pad_token_id\": 50256,\n","  \"resid_dropout\": 0,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.22.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257,\n","  \"window_size\": 256\n","}\n","\n","loading weights file ./gpt2-muchtar-small/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n","\n","All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at ./gpt2-muchtar-small.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/tokenizer.json\n","loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/added_tokens.json\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Norod78--hebrew-gpt_neo-small/snapshots/fd891958cc050222616d6fa5b697bf5d43ff8955/tokenizer_config.json\n"]}],"source":["from transformers import pipeline\n","\n","generator = pipeline('text-generation',model='./gpt2-muchtar-small', tokenizer=model_name)\n","\n","result = generator('Zuerst Hähnchen')[0]['generated_text']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":612,"status":"ok","timestamp":1663439655149,"user":{"displayName":"Lior Moshe","userId":"15921272631253802183"},"user_tz":-180},"id":"duVw1r0ckW6J","outputId":"f66d6b3d-0c43-44a8-e8a6-3508797901ab"},"outputs":[{"data":{"text/plain":["[{'generated_text': '       איילת שקד- תיכנסו  _ Ayelet Shaked\\nאין על המר'}]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["chef('      ')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0o7hz9YkW8g"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dk3Xz8eBkW-l"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4Yf6jpxkXAZ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tsh6tiILkXCv"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQiX00AkkXE0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52zRGUd6kXH0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FdbpZjgrkSll"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9v6YnNVXjxzd"},"source":[]},{"cell_type":"markdown","metadata":{"id":"5IE33NoVbDBK"},"source":["Fine Tune the model over the new data"]},{"cell_type":"markdown","metadata":{"id":"iOPy76IKbrrf"},"source":["Finally train the model"]},{"cell_type":"markdown","metadata":{"id":"C_kfkrszVNQY"},"source":["###Creating the interface and launching!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":802},"executionInfo":{"elapsed":505694,"status":"ok","timestamp":1663439612710,"user":{"displayName":"Lior Moshe","userId":"15921272631253802183"},"user_tz":-180},"id":"rdhGABS3-Gu9","outputId":"83382c39-4c42-4638-be94-cd34767438b3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n","  \"Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\",\n","/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n","  warnings.warn(value)\n"]},{"name":"stdout","output_type":"stream","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Running on public URL: https://13547.gradio.app\n","\n","This share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n"]},{"data":{"text/html":["<div><iframe src=\"https://13547.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  UserWarning,\n","Input length of input_ids is 25, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"]},{"name":"stdout","output_type":"stream","text":["Keyboard interruption in main thread... closing server.\n"]},{"data":{"text/plain":["(<gradio.routes.App at 0x7f0008a484d0>,\n"," 'http://127.0.0.1:7860/',\n"," 'https://13547.gradio.app')"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["output_text = gr.outputs.Textbox()\n","gr.Interface(chef,\"textbox\", output_text, title=model_name,\n","             description=\"Go ahead and input a sentence and see what it completes \\\n","             it with! Takes around 20s to run.\").launch(debug=True)"]},{"cell_type":"markdown","metadata":{"id":"VsYULBhZhc0M"},"source":["#### The model is now live on the gradio.app link shown above. Go ahead and open that in a new tab!"]},{"cell_type":"markdown","metadata":{"id":"xn27MzU0hdS2"},"source":["Please contact us [here](mailto:team@gradio.app) if you have any questions, or [open an issue](https://github.com/gradio-app/gradio-UI/issues/new/choose) at our github repo.\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[{"file_id":"https://github.com/Norod/hebrew-gpt_neo/blob/main/Demo_of_GRADIO2_Hebrew_GPT_Neo.ipynb","timestamp":1663430469514}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}